# -*- coding: utf-8 -*-
"""Task 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MxNIEy1ADmrSffjNzdormRJukSmjG0pA

# Credit risk analysis

create a function which inputs accounts information and outputs expected loss

expected loss = outstanding loan * PD * (1 - RR)
"""

import pandas as pd
import numpy as np
import math

df = pd.read_csv("Task 3 and 4_Loan_Data.csv")

df.describe()

pd.isnull(df).info()

"""## feature engineering"""

df['education'] = np.divide(df['income'], df['years_employed'])
df['payment_pressure'] = np.divide(df['loan_amt_outstanding'], df['income'])
df['ltd_ration'] = np.divide(df['loan_amt_outstanding'], df['total_debt_outstanding'])

df.replace([np.inf, -np.inf], np.nan, inplace=True)

"""## model training

## lightGBM & Xgboost

lightgbm
"""

import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold,GridSearchCV

X = df.drop('default', axis = 1)
y = df['default']

lgbm = lgb.LGBMClassifier(random_state =256,earlystopping = True)

param_grid = {
    'n_estimators':[100,200,500],
    'learning_rate':[0.01,0.05,0.1],
    'num_leaves':[20,32,40],
    'max_depth':[-1,3,5,10],
    'reg_lambda':[0.1,1.0,10.0]
}

cv_strategy = StratifiedKFold(n_splits=5, shuffle =True, random_state = 256)

grid_search = GridSearchCV(
    estimator = lgbm,
    param_grid = param_grid,
    scoring ='roc_auc',
    cv = cv_strategy,
    n_jobs = 1,
    verbose = 100
)

for column in X.columns:
    print(df.groupby('default')[column].describe())
    print("\n" + "="*50 + "\n")

leaky_features = [
    'total_debt_outstanding',
    'ltd_ration',
    'credit_lines_outstanding'
]
df_clean = df.drop(columns=leaky_features)

X = df_clean.drop('default', axis=1)
y = df_clean['default']

grid_search.fit(X,y)
print(f"best params:{grid_search.best_params_}")
print(f"best CV score: {grid_search.best_score_:.4f}")

"""xgboost model"""

import xgboost as xgb

from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split

X_train, X_val, y_train, y_val = train_test_split(
    X,
    y,
    test_size=0.2, # Reserve 20% of the data for validation
    random_state=256,
    stratify=y # Ensure the validation set has the same default ratio as the original data
)

xgboost_model = xgb.XGBClassifier(
    n_estimators=1000,
    use_label_encoder=False,
    eval_metric='auc',
    random_state=256,
    early_stopping_rounds =50
)

param_grid_xgb = {
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'gamma': [0, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=256)

grid_search_xgb = GridSearchCV(
    estimator=xgboost_model,
    param_grid=param_grid_xgb,
    scoring='roc_auc',
    cv=cv_strategy,
    n_jobs=-1,
    verbose=50
)
fit_params = {
    "eval_set": [(X_val, y_val)],
    "verbose": False
}

grid_search_xgb.fit(X_train, y_train, **fit_params)

print(f"est Parameters: {grid_search_xgb.best_params_}")

print(f"Best CV Score: {grid_search_xgb.best_score_:.4f}")

best_xgb_model = grid_search_xgb.best_estimator_
print(f"Best Iteration: {best_xgb_model.best_iteration}")

"""## regression & decision tree"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

leaky_features = [
    'total_debt_outstanding',
    'ltd_ration',
    'credit_lines_outstanding'
]
df_clean = df.drop(columns=leaky_features)

X = df_clean.drop('default', axis=1)
y = df_clean['default']

cv_strategy = StratifiedKFold(n_splits= 5, shuffle=True, random_state= 256)

decision_tree = DecisionTreeClassifier(max_depth = 3, random_state= 256)

dt_scores = cross_val_score(decision_tree, X, y, cv=cv_strategy,scoring ='roc_auc')

print(f"decision tree 5 fold ROC AUC score: {dt_scores}")
print(f"decision tree average ROC AUC score: {dt_scores.mean():.4f}")

decision_tree.fit(X,y)

plt.figure(figsize=(20, 10))
plot_tree(decision_tree,
          feature_names=X.columns,
          class_names=['Not Default', 'Default'],
          filled=True,
          rounded=True,
          fontsize=10)
plt.title("Visualization of the Decision Tree")
plt.show()

"""logistic regression"""

from sklearn.impute import SimpleImputer
logistic_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('log_reg', LogisticRegression(random_state=42, max_iter=1000))
])

lr_scores = cross_val_score(logistic_pipeline, X, y, cv=cv_strategy, scoring='roc_auc')

print(f"logistic regression 5 fold ROC AUC score: {lr_scores}")
print(f"logistic regression average ROC AUC score: {lr_scores.mean():.4f}")

logistic_pipeline.fit(X,y)
log_reg_model = logistic_pipeline.named_steps['log_reg']
coeff_data = {
    'Feature': X.columns,
    'Coefficient': log_reg_model.coef_[0]
}

coefficients_df = pd.DataFrame(coeff_data)
coefficients = coefficients_df.sort_values('Coefficient', ascending=False)

print("\n coefficients:")
print(coefficients)

"""# model assembling and validation"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import StackingClassifier

import lightgbm as lgb
import xgboost as xgb

lr_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('log_reg', LogisticRegression(random_state=42, max_iter=1000))
])


dt_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('tree', DecisionTreeClassifier(max_depth=4, random_state=42))
])


lgbm_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('lgbm', lgb.LGBMClassifier(learning_rate=0.05, max_depth=3, n_estimators=100,
                                 num_leaves=20, reg_lambda=10.0, random_state=42))
])


xgb_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('xgb', xgb.XGBClassifier(learning_rate=0.05, max_depth=3, n_estimators=100,
                              gamma=0.1, subsample=0.8, colsample_bytree=0.8,
                              use_label_encoder=False, eval_metric='logloss', random_state=42))
])

base_models = [
    ('logistic_regression', lr_pipeline),
    ('decision_tree', dt_pipeline),
    ('lightgbm', lgbm_pipeline),
    ('xgboost', xgb_pipeline)
]

meta_model = LogisticRegression(random_state=256)

stacking_classifier = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    passthrough=True,
    cv=5
)

master_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('stacking', stacking_classifier) ])

outer_cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=256)

stacking_scores = cross_val_score(
    master_pipeline, # <-- Use the full pipeline here
    X,
    y,
    cv=outer_cv_strategy,
    scoring='roc_auc'
)

print("\nCross-validation finished successfully!")
print(f"Stacking Ensemble 5-Fold ROC AUC Scores: {stacking_scores}")
print(f"Average ROC AUC Score: {stacking_scores.mean():.4f}")
print(f"Standard Deviation of Scores: {stacking_scores.std():.4f}")

"""logistic model performs better than any other models, even the assembled model."""

def predict_expected_loss(borrowers_list, trained_model, RR = 0.1):
  input_df = pd.DataFrame(borrowers_list)

  input_df['education'] = np.divide(input_df['income'], input_df['years_employed'])
  input_df['payment_pressure'] = np.divide(input_df['loan_amt_outstanding'], input_df['income'])
  input_df.replace([np.inf, -np.inf], np.nan, inplace=True)

  leaky_features = [
    'total_debt_outstanding',
    'credit_lines_outstanding']

  input_df = input_df.drop(columns=leaky_features)

  input_df = input_df[X.columns]

  prob_defaults = trained_model.predict_proba(input_df)[:, 1]

  loan_amount = input_df['loan_amt_outstanding']
  LGD = 1 - RR
  expected_losses = prob_defaults * LGD * loan_amount

  results_df = pd.DataFrame(borrowers_list)
  results_df['prob_default'] = prob_defaults
  results_df['expected_loss'] = expected_losses

  return results_df



new_borrowers_list = [
    {
        'customer_id': 9999999,
        'loan_amt_outstanding': 25000,
        'income': 85000,
        'years_employed': 5,
        'fico_score': 680,
        'total_debt_outstanding':10000,
        'credit_lines_outstanding':10000
    },
    {
        'customer_id': 8888888,
        'loan_amt_outstanding': 40000,
        'income': 60000,
        'years_employed': 1,
        'fico_score': 590,
        'total_debt_outstanding':10000,
        'credit_lines_outstanding':10000
    },
    {
        'customer_id': 7777777,
        'loan_amt_outstanding': 10000,
        'income': 120000,
        'years_employed': 15,
        'fico_score': 780,
        'total_debt_outstanding':10000,
        'credit_lines_outstanding':10000
    }
]


batch_results = predict_expected_loss(new_borrowers_list, logistic_pipeline)

print("expected losses prediction:")
print(batch_results)